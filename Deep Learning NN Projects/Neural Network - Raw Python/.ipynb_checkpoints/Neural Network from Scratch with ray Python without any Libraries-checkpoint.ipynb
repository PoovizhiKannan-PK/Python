{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coding a Single neuron that take input from 3 neurons.\n",
    "\n",
    "inputs = [1, 2, 3.5]      # inputs from 3 neurons\n",
    "weights = [2.7, 3.5, 4.2] # weights associated with 3 links from 3 neurosns\n",
    "bias = 3                  # bias associated with the neuron that's taking the input\n",
    "\n",
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "# Coding a 3 neuron layer with each neuron taking input from 4 neurons from previous 4 neuron layer\n",
    "\n",
    "inputs = [1.0, 2.0, 3.0, 2.5]              # The input from 4 neurons will be same for all 3 neurons in this layer\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],           # The weights associated with each link will vary\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2.0, 3.0, 0.5]\n",
    "\n",
    "output = []\n",
    "\n",
    "for b, wht in zip(biases, weights):\n",
    "    product_sum = 0\n",
    "    for i, w in zip(inputs, wht):\n",
    "        product_sum += i*w\n",
    "    output.append(product_sum + b)\n",
    "    \n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (4,) and (3,4) not aligned: 4 (dim 0) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b13be5d8e757>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbiases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (4,) and (3,4) not aligned: 4 (dim 0) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "output = np.dot(inputs, weights) + biases\n",
    "print(output)\n",
    "\n",
    "# Why error? See Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8   1.21  2.385]\n"
     ]
    }
   ],
   "source": [
    "# Now the dot product. Easier way of doing the above\n",
    "\n",
    "output = np.dot(weights, inputs) + biases\n",
    "print(output)\n",
    "\n",
    "# Dot prooduct is matrix multiplication -> weights is 3*4 matrix, inputs 1*4 vectorb. \n",
    "# For two matrices to be dot product compatible, both matrix must have same number of column\n",
    "# output will be of rows of 2st matrix * rows of 1nd matrix, matrix. \n",
    "# High school, College math is coming in hand, who would have thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n"
     ]
    }
   ],
   "source": [
    "# With 3 input sets, i.e., inputs from 3 different records\n",
    "# Here the multiplication follows matrix multiplication\n",
    "\n",
    "inputs = [[1.0, 2.0, 3.0, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],          \n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2.0, 3.0, 0.5]\n",
    "\n",
    "# Matrix mul of 3*4 and 3*4 is not possible. You know y, u read it in school.\n",
    "# So, you are transposing the weights to 4*3. Now 3*4 x 4*3 will give 3*3\n",
    "\n",
    "# output = np.dot(inputs, np.array(weights).T) #+ biases\n",
    "# print(output)\n",
    "\n",
    "# output1 = np.dot(weights, np.array(inputs).T) #+ biases\n",
    "# print(output1)\n",
    "\n",
    "output = np.dot(inputs, np.array(weights).T) + biases\n",
    "print(output)\n",
    "\n",
    "# output1 = np.dot(weights, np.array(inputs).T + biases)\n",
    "# print(output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5031  -1.04185 -2.03875]\n",
      " [ 0.2434  -2.7332  -5.7633 ]\n",
      " [-0.99314  1.41254 -0.35655]]\n"
     ]
    }
   ],
   "source": [
    "inputs = [[1.0, 2.0, 3.0, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],          \n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2.0, 3.0, 0.5]\n",
    "\n",
    "weights2 = [[0.1, -0.14, 0.5],          \n",
    "           [-0.5, 0.12, -0.33],\n",
    "           [-0.44, 0.73, -0.13]]\n",
    "\n",
    "biases2 = [-1.0, 2.0, -0.5]\n",
    "\n",
    "\n",
    "layer1_output = np.dot(inputs, np.array(weights).T) + biases\n",
    "layer2_output = np.dot(layer1_output, np.array(weights2).T) + biases2  # adding second layer\n",
    "\n",
    "print(layer2_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10758131  1.03983522  0.24462411  0.31821498  0.18851053]\n",
      " [-0.08349796  0.70846411  0.00293357  0.44701525  0.36360538]\n",
      " [-0.50763245  0.55688422  0.07987797 -0.34889573  0.04553042]]\n",
      "\n",
      "\n",
      "<------------->\n",
      "\n",
      "\n",
      "[[ 0.148296   -0.08397602]\n",
      " [ 0.14100315 -0.01340469]\n",
      " [ 0.20124979 -0.07290616]]\n"
     ]
    }
   ],
   "source": [
    "# So now we'll start building a NN with random weights and bises with OOP.\n",
    "# We ususaly keep weights range between -0.1 to 0.1, so that the values o/p by neurons don't become too large, called exploding\n",
    "# And we start of the biases with 0. If for 0 biase, the output is all zero, then the NN is deaad that means start the bias with\n",
    "# a non zero number.\n",
    "# We'll use the same input as above\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "X = [[1.0, 2.0, 3.0, 2.5],            # input sample set. 3 input sets with 4 features each.\n",
    "     [2.0, 5.0, -1.0, 2.0], \n",
    "     [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "class Layer_Dense:\n",
    "    \n",
    "    def __init__(self, n_inputs, n_neurons):                 # n_inputs is the size of single set in X, n is no. of neurons  \n",
    "        self.weights = 0.1*np.random.randn(n_inputs, n_neurons)  # here for randn, 1st parameter is input size and 2nd is no. of neurons\n",
    "        self.biases = np.zeros((1, n_neurons))              # here for np.zeroes, the first parameter itself if tuple of shape.\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "layer1 = Layer_Dense(4, 5)   # n_inputs is 4 beacuse there are 4 inputs in each of 3 sets in X, we set n_neurons to be 5\n",
    "layer2 = Layer_Dense(5, 2)   # n_inputs is 5 beacuse 5 output from layer 1, here we set n_neurons to be 2\n",
    "\n",
    "layer1.forward(X)\n",
    "print(layer1.output)       # for 3 sample set, three outputs with 5 neurons each\n",
    "\n",
    "print('\\n\\n<------------->\\n\\n')\n",
    "\n",
    "layer2.forward(layer1.output)\n",
    "print(layer2.output)            # for 3 sample set, three outputs with 2 neurons each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "\n",
    "\n",
    "Input to the activation function is the result from ((input*weight) + biase)\n",
    "\n",
    "Step Function --> Gives output as 1 if input is > 0, else 0\n",
    "\n",
    "Sigmoid Function --> \n",
    "y = 1/(1+(e^-1))\n",
    "\n",
    "Why sigmoid over step funtion?\n",
    "Output from sigmoid is more granular than step function i., with sigmoid we can determine with given input how close the output get to 1. One main issue with sigmoid is something called vanishing gradient problem.\n",
    "\n",
    "Rectified Linear Unit --> If input is > 0, then the output is input. If input <= 0, then output is 0\n",
    "y = x, if x > 0\n",
    "y = 0, if x <= 0\n",
    "\n",
    "Why relu over sigmoid?\n",
    "As granular as sigmoid, but the calculation of relu is way simpler than sigmoid, so is much faster than sigmoid.\n",
    "RElu is most commonly used activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnfs\n",
    "from nnfs.datasets import spiral_data  # See for code: https://gist.github.com/Sentdex/454cb20ec5acf0e76ee8ab8448e6266c\n",
    "\n",
    "nnfs.init()        # Similar to setting a seed value\n",
    "X, y = spiral_data(100, 3) \n",
    "\n",
    "class Layer_Dense:\n",
    "    \n",
    "    def __init__(self, n_inputs, n_neurons):                 \n",
    "        self.weights = 0.1*np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "layer1 = Layer_Dense(4, 5)   \n",
    "layer2 = Layer_Dense(5, 2)\n",
    "\n",
    "layer1.forward(X)\n",
    "print(layer1.output)      \n",
    "\n",
    "print('\\n\\n<------------->\\n\\n')\n",
    "\n",
    "layer2.forward(layer1.output)\n",
    "print(layer2.output)           "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
