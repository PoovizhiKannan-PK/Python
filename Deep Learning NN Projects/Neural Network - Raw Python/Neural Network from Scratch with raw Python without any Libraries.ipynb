{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coding a Single neuron that take input from 3 neurons.\n",
    "\n",
    "inputs = [1, 2, 3.5]      # inputs from 3 neurons\n",
    "weights = [2.7, 3.5, 4.2] # weights associated with 3 links from 3 neurosns\n",
    "bias = 3                  # bias associated with the neuron that's taking the input\n",
    "\n",
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "# Coding a 3 neuron layer with each neuron taking input from 4 neurons from previous 4 neuron layer\n",
    "\n",
    "inputs = [1.0, 2.0, 3.0, 2.5]              # The input from 4 neurons will be same for all 3 neurons in this layer\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],           # The weights associated with each link will vary\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2.0, 3.0, 0.5]\n",
    "\n",
    "output = []\n",
    "\n",
    "for b, wht in zip(biases, weights):\n",
    "    product_sum = 0\n",
    "    for i, w in zip(inputs, wht):\n",
    "        product_sum += i*w\n",
    "    output.append(product_sum + b)\n",
    "    \n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (4,) and (3,4) not aligned: 4 (dim 0) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b13be5d8e757>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbiases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (4,) and (3,4) not aligned: 4 (dim 0) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "output = np.dot(inputs, weights) + biases\n",
    "print(output)\n",
    "\n",
    "# Why error? See Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8   1.21  2.385]\n"
     ]
    }
   ],
   "source": [
    "# Now the dot product. Easier way of doing the above\n",
    "\n",
    "output = np.dot(weights, inputs) + biases\n",
    "print(output)\n",
    "\n",
    "# Dot prooduct is matrix multiplication -> weights is 3*4 matrix, inputs 1*4 vectorb. \n",
    "# For two matrices to be dot product compatible, both matrix must have same number of column\n",
    "# output will be of rows of 2st matrix * rows of 1nd matrix, matrix. \n",
    "# High school, College math is coming in hand, who would have thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n"
     ]
    }
   ],
   "source": [
    "# With 3 input sets, i.e., inputs from 3 different records\n",
    "# Here the multiplication follows matrix multiplication\n",
    "\n",
    "inputs = [[1.0, 2.0, 3.0, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],          \n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2.0, 3.0, 0.5]\n",
    "\n",
    "# Matrix mul of 3*4 and 3*4 is not possible. You know y, u read it in school.\n",
    "# So, you are transposing the weights to 4*3. Now 3*4 x 4*3 will give 3*3\n",
    "\n",
    "# output = np.dot(inputs, np.array(weights).T) #+ biases\n",
    "# print(output)\n",
    "\n",
    "# output1 = np.dot(weights, np.array(inputs).T) #+ biases\n",
    "# print(output1)\n",
    "\n",
    "output = np.dot(inputs, np.array(weights).T) + biases\n",
    "print(output)\n",
    "\n",
    "# output1 = np.dot(weights, np.array(inputs).T + biases)\n",
    "# print(output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5031  -1.04185 -2.03875]\n",
      " [ 0.2434  -2.7332  -5.7633 ]\n",
      " [-0.99314  1.41254 -0.35655]]\n"
     ]
    }
   ],
   "source": [
    "inputs = [[1.0, 2.0, 3.0, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],          \n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2.0, 3.0, 0.5]\n",
    "\n",
    "weights2 = [[0.1, -0.14, 0.5],          \n",
    "           [-0.5, 0.12, -0.33],\n",
    "           [-0.44, 0.73, -0.13]]\n",
    "\n",
    "biases2 = [-1.0, 2.0, -0.5]\n",
    "\n",
    "\n",
    "layer1_output = np.dot(inputs, np.array(weights).T) + biases\n",
    "layer2_output = np.dot(layer1_output, np.array(weights2).T) + biases2  # adding second layer\n",
    "\n",
    "print(layer2_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10758131  1.03983522  0.24462411  0.31821498  0.18851053]\n",
      " [-0.08349796  0.70846411  0.00293357  0.44701525  0.36360538]\n",
      " [-0.50763245  0.55688422  0.07987797 -0.34889573  0.04553042]]\n",
      "\n",
      "\n",
      "<------------->\n",
      "\n",
      "\n",
      "[[ 0.148296   -0.08397602]\n",
      " [ 0.14100315 -0.01340469]\n",
      " [ 0.20124979 -0.07290616]]\n"
     ]
    }
   ],
   "source": [
    "# So now we'll start building a NN with random weights and bises with OOP.\n",
    "# We ususaly keep weights range between -0.1 to 0.1, so that the values o/p by neurons don't become too large, called exploding\n",
    "# And we start of the biases with 0. If for 0 biase, the output is all zero, then the NN is deaad that means start the bias with\n",
    "# a non zero number.\n",
    "# We'll use the same input as above\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "X = [[1.0, 2.0, 3.0, 2.5],            # input sample set. 3 input sets with 4 features each.\n",
    "     [2.0, 5.0, -1.0, 2.0], \n",
    "     [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "class Layer_Dense:\n",
    "    \n",
    "    def __init__(self, n_inputs, n_neurons):                 # n_inputs is the size of single set in X, n is no. of neurons  \n",
    "        self.weights = 0.1*np.random.randn(n_inputs, n_neurons)  # here for randn, 1st parameter is input size and 2nd is no. of neurons\n",
    "        self.biases = np.zeros((1, n_neurons))              # here for np.zeroes, the first parameter itself if tuple of shape.\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "layer1 = Layer_Dense(4, 5)   # n_inputs is 4 beacuse there are 4 inputs in each of 3 sets in X, we set n_neurons to be 5\n",
    "layer2 = Layer_Dense(5, 2)   # n_inputs is 5 beacuse 5 output from layer 1, here we set n_neurons to be 2\n",
    "\n",
    "layer1.forward(X)\n",
    "print(layer1.output)       # for 3 sample set, three outputs with 5 neurons each\n",
    "\n",
    "print('\\n\\n<------------->\\n\\n')\n",
    "\n",
    "layer2.forward(layer1.output)\n",
    "print(layer2.output)            # for 3 sample set, three outputs with 2 neurons each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "\n",
    "\n",
    "Input to the activation function is the result from ((input*weight) + biase)\n",
    "\n",
    "Step Function --> Gives output as 1 if input is > 0, else 0\n",
    "\n",
    "Sigmoid Function --> \n",
    "y = 1/(1+(e^-1))\n",
    "\n",
    "Why sigmoid over step funtion?\n",
    "Output from sigmoid is more granular than step function i., with sigmoid we can determine with given input how close the output get to 1. One main issue with sigmoid is something called vanishing gradient problem.\n",
    "\n",
    "Rectified Linear Unit --> If input is > 0, then the output is input. If input <= 0, then output is 0\n",
    "y = x, if x > 0\n",
    "y = 0, if x <= 0\n",
    "\n",
    "Why relu over sigmoid?\n",
    "As granular as sigmoid, but the calculation of relu is way simpler than sigmoid, so is much faster than sigmoid.\n",
    "RElu is most commonly used activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [-8.35815910e-04 -7.90404272e-04 -1.33452227e-03  4.65504505e-04\n",
      "   4.56846210e-05]\n",
      " [-2.39994470e-03  5.93469958e-05 -2.24808278e-03  2.03573116e-04\n",
      "   6.10024377e-04]\n",
      " ...\n",
      " [ 1.13291524e-01 -1.89262271e-01 -2.06855070e-02  8.11079666e-02\n",
      "  -6.71350807e-02]\n",
      " [ 1.34588361e-01 -1.43197834e-01  3.09493970e-02  5.66337556e-02\n",
      "  -6.29687458e-02]\n",
      " [ 1.07817926e-01 -2.00809643e-01 -3.37579325e-02  8.72561932e-02\n",
      "  -6.81458861e-02]]\n",
      "\n",
      "\n",
      "\n",
      "<--------------->\n",
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 4.65504505e-04\n",
      "  4.56846210e-05]\n",
      " [0.00000000e+00 5.93469958e-05 0.00000000e+00 2.03573116e-04\n",
      "  6.10024377e-04]\n",
      " ...\n",
      " [1.13291524e-01 0.00000000e+00 0.00000000e+00 8.11079666e-02\n",
      "  0.00000000e+00]\n",
      " [1.34588361e-01 0.00000000e+00 3.09493970e-02 5.66337556e-02\n",
      "  0.00000000e+00]\n",
      " [1.07817926e-01 0.00000000e+00 0.00000000e+00 8.72561932e-02\n",
      "  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data  # See for code: https://gist.github.com/Sentdex/454cb20ec5acf0e76ee8ab8448e6266c\n",
    "                                       #from https://cs231n.github.io/neural-networks-case-study/ \n",
    "\n",
    "nnfs.init()        # Similar to setting a seed value\n",
    "X, y = spiral_data(100, 3) # so 3 classes with 100 inputs will be retured. This input is x,y coordinate of dots. So each input\n",
    "                           # will two features i.e., x coordinate and y coordinate.\n",
    "\n",
    "\n",
    "class Layer_Dense:\n",
    "    \n",
    "    def __init__(self, n_inputs, n_neurons):                 \n",
    "        self.weights = 0.1*np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "layer1 = Layer_Dense(2, 5)  # 2 beacuse two feature per input. 5 denotes the neurons\n",
    "layer1.forward(X)\n",
    "print(layer1.output)\n",
    "activation = Activation_ReLU()\n",
    "activation.forward(layer1.output)\n",
    "print('\\n\\n\\n<--------------->')\n",
    "print(activation.output)    # So all the negetive inputs are changed to zero.  \n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Activation Function:\n",
    "\n",
    "Why softmax for last output layer, why not just relu?\n",
    "Well, while training a neural network it is important to see how close is each output to the actual value i.e., calculate loss, back propogate it to the NN to increase accuracy. So, the first reason isReLU is associated with one neuron, i., input to one neuron and output to that neuron, while for a output we need a probability distribution across each class/op neuron, so as to train the model. The second reason is, relu converts all -ve outputs to 0. So if a set of inputs give -0.00056 and another gives -90000.6, even though the actual diff very very high, ReLU will convert both to 0 and we'll never know which is closer to actual output. Third reason is, what if all the outputs are negetive values, then everthing will become zero and NN will be dead.\n",
    "\n",
    "What's in softmax function?\n",
    "\n",
    "\n",
    "- In softmax function, first we exponentiate all the values, to convert -ve to +ve values without loosing the meaning to the -ve values.\n",
    "\n",
    "y = e^x, where e is the euiler's number. e = 2.718281528459045\n",
    "So this function, for negetive values it'll give a very low positive value, and exponentially increases as it goes up.\n",
    "\n",
    "- Then we normalize the output values i.e., we divide each output by sum of all the outputs, so as to get the probability distribution among the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33334148 0.33333018 0.33332834]\n",
      " [0.33335316 0.33332598 0.3333209 ]\n",
      " [0.333332   0.3333076  0.3333604 ]\n",
      " [0.33333603 0.33330086 0.33336315]]\n"
     ]
    }
   ],
   "source": [
    "# Implementing softmax\n",
    "\n",
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data \n",
    "                                       \n",
    "nnfs.init()        \n",
    " \n",
    "class Layer_Dense:\n",
    "    \n",
    "    def __init__(self, n_inputs, n_neurons):                 \n",
    "        self.weights = 0.1*np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "class ActivationReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "class ActivationSoftmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True)) \n",
    "        # Taking exponential of inputs.\n",
    "        # inputs - max of inputs, to save the exp output from exploding and doing this will keep the values between 0 and 1.\n",
    "        # np.max(axis = None #default value) will give max number from all the batches. \n",
    "        # np.max(axis = 1) specifies to take the max value from each batch.\n",
    "        # The batches will be of dimension, (1 * no. of batches). In this case we cannot subtract eah batch of input values with\n",
    "        # the max value specific to that batch.\n",
    "        # np.max(keepdims = True) will convert the dimentions to (no. of batches * 1), too achevie the above.\n",
    "        \n",
    "        probabilities = exp_values/np.sum(exp_values, axis=1, keepdims = True)\n",
    "        self.output = probabilities\n",
    "        \n",
    "\n",
    "X, y = spiral_data(samples = 100, classes = 3)   \n",
    "\n",
    "layer1 = Layer_Dense(2, 5)\n",
    "activation1 = ActivationReLU()\n",
    "\n",
    "layer2 = Layer_Dense(5, 3)  # output is 3 classes\n",
    "activation2 = ActivationSoftmax()\n",
    "\n",
    "layer1.forward(X)\n",
    "activation1.forward(layer1.output)\n",
    "layer2.forward(activation1.output)\n",
    "activation2.forward(layer2.output)\n",
    "\n",
    "#print(activation2.output)\n",
    "\n",
    "print(activation2.output[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions:\n",
    "\n",
    "Categorical Cross-Entropy -> -ve(Sum(softmax_output\\[i\\] * one_hot_vector\\[i\\]))\n",
    "\n",
    "One Hot Vector-> It is a vector of size of number of target classes in the model, with the target class index value 1 and all other index values 0.\n",
    "\n",
    "Let take for a model there are three target classes.\n",
    "\n",
    "target classes = \\[cat, dog, squriel\\]\n",
    "\n",
    "- for input of image x1 with label cat --> one hot vector = \\[1, 0, 0\\]\n",
    "- for input of image x2 with label dog --> one hot vector = \\[0, 1, 0\\]\n",
    "- for input of image x1 with label squriels --> one hot vector = \\[0, 0, 1\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n",
      "0.35667494393873245\n",
      "0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "softmax_output = [0.7, 0.1, 0.2]     \n",
    "target_output = [1, 0, 0]            # one_hot_vector\n",
    "\n",
    "loss = -(math.log(softmax_output[0]) * target_output[0] +\n",
    "         math.log(softmax_output[1]) * target_output[1] +\n",
    "         math.log(softmax_output[2]) * target_output[2])\n",
    "\n",
    "print(loss)\n",
    "\n",
    "print(-math.log(0.7))  # If the prediction is close to one, the loss is less\n",
    "print(-math.log(0.5))  # If the prediction is close to zero, the loss is high as seen below"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
